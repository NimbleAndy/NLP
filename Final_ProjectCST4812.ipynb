{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.functional import F\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, word2int, embedding_dim, n_hidden, n_layers, output_size=1, drop_prob=0.5):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.word2int = word2int\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_layers = n_layers\n",
        "        self.vocab_size = len(word2int) + 1\n",
        "        self.embedding = nn.Embedding(self.vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, n_hidden, n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden * 2, output_size)  # n_hidden * 2 because it's bidirectional\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # Set the device attribute I use cpu, don't have a gpu....yet\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)  # Move model to appropriate device\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x, hidden = self.lstm(x, hidden)\n",
        "        x = x[:, -1]  # Take the output from the last timestep\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (\n",
        "            weight.new(self.n_layers * 2, batch_size, self.n_hidden).zero_().to(self.device),\n",
        "            weight.new(self.n_layers * 2, batch_size, self.n_hidden).zero_().to(self.device)\n",
        "        )\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize the weights using uniform distribution, learned this from YT\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "        # Initialize the weights using 0\n",
        "        self.fc.bias.data.fill_(0)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "rjVe_2hyIN3E"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process():\n",
        "    # Read the reviews.txt and labels.txt, then store them in variables, dataset\n",
        "    with open('/content/reviews.txt', 'r') as f:\n",
        "        reviews_text = f.read()\n",
        "\n",
        "    with open('/content/labels.txt', 'r') as f:\n",
        "        sentiment = f.read()\n",
        "\n",
        "    # Convert the reviews text to lowercase,\n",
        "    # then remove all the punctuations\n",
        "    reviews_text = reviews_text.lower()\n",
        "    reviews_text = ''.join([c for c in reviews_text if c not in punctuation])\n",
        "    print(reviews_text)\n",
        "\n",
        "    # Each line contains one review. Create a list of reviews.\n",
        "    reviews_list = reviews_text.split('\\n')\n",
        "\n",
        "    # Merge all the text in one string\n",
        "    # This will be required for creating the char2int dict\n",
        "    words = (' '.join(reviews_list)).split()\n",
        "\n",
        "    # Build a dictionary that maps words to integers\n",
        "    # Using Counter to set the values as number of word occurrences\n",
        "    # High frequency words will have high occurrences\n",
        "    counts = Counter(words)\n",
        "\n",
        "    # Sort the dict based on # of occurrences from most to least\n",
        "    sorted_dict = sorted(counts, key=counts.get, reverse=True)\n",
        "\n",
        "    # Create the word2int dict with most frequent word starting from index 1\n",
        "    # Reserve 0 for padding\n",
        "    word2int = {word: index for index, word in enumerate(sorted_dict, 1)}\n",
        "\n",
        "    # Encode the reviews text using word2int\n",
        "    reviews_encoded = []\n",
        "    for review in reviews_list:\n",
        "        reviews_encoded.append([word2int[word] for word in review.split()])\n",
        "\n",
        "    # Convert the labels to 0/1\n",
        "    labels_encoded = []\n",
        "    sentiment_list = sentiment.split('\\n')\n",
        "    for sentiment in sentiment_list:\n",
        "        if sentiment == 'positive':\n",
        "            labels_encoded.append(1)\n",
        "        else:\n",
        "            labels_encoded.append(0)\n",
        "\n",
        "    return reviews_encoded, labels_encoded, word2int\n",
        "\n"
      ],
      "metadata": {
        "id": "cI4SJUA0ISkl"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_reviews(reviews_encoded, labels_encoded):\n",
        "    # Remove entries with empty reviews, only interested in text so empty str aren't helpful\n",
        "    count = 0\n",
        "    for i, review in enumerate(reviews_encoded):\n",
        "        if len(review) == 0:\n",
        "            count = 1\n",
        "            del reviews_encoded[i]\n",
        "            del labels_encoded[i]\n",
        "\n",
        "    print(\"Removed %d reviews\" % count)\n",
        "\n",
        "    # Convert the labels_encoded from list to Numpy Array\n",
        "    labels_ndarray = np.array(labels_encoded)\n",
        "\n",
        "    return reviews_encoded, labels_ndarray\n"
      ],
      "metadata": {
        "id": "NYMUnRs_IhxB"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(reviews_encoded, seq_length):\n",
        "\n",
        "    features = list()\n",
        "\n",
        "    # Loop through the reviews\n",
        "    for i, reviews in enumerate(reviews_encoded):\n",
        "\n",
        "        if len(reviews) > seq_length:\n",
        "            # cut off point in reviews\n",
        "            features.append(reviews[0:seq_length])\n",
        "        elif len(reviews) < seq_length:\n",
        "            # Calculate padding and prepend, every input needs to be uniform\n",
        "            difference = seq_length - len(reviews)\n",
        "            arr = [0 for i in range(difference)]\n",
        "            arr.extend(reviews)\n",
        "            features.append(arr)\n",
        "        else:\n",
        "            features.append(reviews)\n",
        "\n",
        "    return np.array(features)\n"
      ],
      "metadata": {
        "id": "Etljlw3lIj-3"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_traing_test_val_set(reviews_ndarray, labels_ndarray, train_frac=0.8):\n",
        "\n",
        "    # Find the index to split, in this case it 80% to train...\n",
        "    split = int(len(reviews_ndarray) * train_frac)\n",
        "\n",
        "    #get training data\n",
        "    train_x = reviews_ndarray[:split, :]\n",
        "    train_y = labels_ndarray[:split]\n",
        "\n",
        "    remaining_x = reviews_ndarray[split:, :]\n",
        "    remaining_y = labels_ndarray[split:]\n",
        "\n",
        "    # Split the remaining data in half, rememeber the remaining will be allocated to validation and testing(please refer to the report)\n",
        "    split = int(len(remaining_x) * 0.5)\n",
        "\n",
        "    # Use first half for validation\n",
        "    val_x = remaining_x[:split, :]\n",
        "    val_y = remaining_y[:split]\n",
        "\n",
        "    # Use 2nd half for Testing\n",
        "    test_x = remaining_x[split:, :]\n",
        "    test_y = remaining_y[split:]\n",
        "\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
        "\n"
      ],
      "metadata": {
        "id": "Q6lKI-vfInXr"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(train_x, train_y, val_x, val_y, test_x, test_y, batch_size):\n",
        "\n",
        "    # Create TensorDataset, read this on Pytorch documentation, this is why we used numpy arrays from before\n",
        "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "    val_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "    # Use the DataLoader. Also shuffle the data\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "    val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "9A_QefhVIrKY"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: SentimentRNN, train_loader, val_loader, batch_size=25, epochs=60, lr=0.1, clip=5, print_every=100):\n",
        "\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Define optimization process\n",
        "    optimization = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Define Loss function as Binary Cross Entropy, geeks for geeks said this would be optimal....:/\n",
        "    error_func = nn.BCELoss()\n",
        "\n",
        "    if torch.cuda.is_available():#remember first seeing this in class assignment\n",
        "        model.cuda()\n",
        "\n",
        "    training_loss = []\n",
        "    validation_loss = [] #data struct to store our value\n",
        "\n",
        "    # Loop through the epochs\n",
        "    for i in range(epochs):\n",
        "\n",
        "        # Initialize the hidden layers\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        # Loop though the batches [ Mini-Batch SGD ]\n",
        "        for counter, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            # Move the input and targets to GPU ( if available)\n",
        "            if torch.cuda.is_available():\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history, this took awhile for me to get right....\n",
        "            hidden = tuple([each.data for each in hidden])\n",
        "\n",
        "            # Remove the gradients from the model, minimize it!!\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output, hidden = model.forward(inputs, hidden)\n",
        "\n",
        "            # Calculate the Loss\n",
        "            loss = error_func(output.squeeze(), labels.float())\n",
        "\n",
        "            # Back pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping ( needed to avoid exploding gradients )\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            optimization.step()\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                # Calculate validation loss\n",
        "\n",
        "                val_hc = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "\n",
        "                # Set the model to evaluation state\n",
        "                model.eval()\n",
        "\n",
        "                for inputs, labels in val_loader:\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_hc = tuple([each.data for each in val_hc])\n",
        "\n",
        "                    # Move the input and targets to GPU ( if available )\n",
        "                    if torch.cuda.is_available():\n",
        "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                    # Forward Propagation\n",
        "                    output, val_hc = model.forward(inputs, val_hc)\n",
        "\n",
        "                    val_loss = error_func(output.squeeze(), labels.float())\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                # Set the model to training mode again\n",
        "                model.train()\n",
        "                print(\"Epoch: {}/{}...\".format(i + 1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "        validation_loss.append(np.mean(val_losses))\n",
        "        training_loss.append(loss.item())\n",
        "\n"
      ],
      "metadata": {
        "id": "C08kz7jCIvNo"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model: SentimentRNN, test_loader, batch_size=50):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "        train_on_gpu = True\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.eval()\n",
        "\n",
        "    # Define Loss function as Binary Cross Entropy\n",
        "    error_func = nn.BCELoss()\n",
        "\n",
        "    h = model.init_hidden(batch_size)\n",
        "    test_losses = []\n",
        "\n",
        "    # Set the model to evaluation state\n",
        "    model.eval()\n",
        "\n",
        "    num_correct = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # Move the input and targets to GPU ( if available )\n",
        "        if torch.cuda.is_available():\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Forward Propagation\n",
        "        output, h = model.forward(inputs, h)\n",
        "\n",
        "        test_loss = error_func(output.squeeze(), labels.float())\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "        # convert output probabilities to predicted class (0 or 1)\n",
        "        predictions = torch.round(output.squeeze())\n",
        "\n",
        "        # Compare predictions with true labels\n",
        "        correct_tensor = predictions.eq(labels.float().view_as(predictions))\n",
        "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        num_correct += np.sum(correct)\n",
        "\n",
        "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "    # accuracy over all test data\n",
        "    test_acc = num_correct / len(test_loader.dataset)\n",
        "    print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
      ],
      "metadata": {
        "id": "EbLmrNBFI2Wo"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, input_text):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_text = input_text.lower()\n",
        "        input_text = ''.join([char for char in input_text if char not in punctuation])\n",
        "        inputs = [model.word2int.get(word, 0) for word in input_text.split()]\n",
        "        inputs = torch.tensor(inputs, dtype=torch.long).unsqueeze(0)\n",
        "        inputs = inputs.to(model.device)\n",
        "\n",
        "        hidden = model.init_hidden(1)\n",
        "        outputs, _ = model(inputs, hidden)\n",
        "\n",
        "        if outputs.ndim == 3:  # In case of sequence outputs\n",
        "            outputs = outputs[:, -1, :]  # Select the last timestep\n",
        "        outputs = outputs.squeeze()  # Reduce to 1D tensor if needed\n",
        "        pred_prob = torch.sigmoid(outputs)\n",
        "        pred_label = torch.round(pred_prob).item()\n",
        "\n",
        "        print(f'Raw output (pre-sigmoid): {outputs.item()}')\n",
        "        print(f'Prediction value, pre-rounding: {pred_prob.item():.6f}')\n",
        "\n",
        "        return \"Positive\" if pred_label == 1 else \"Negative\"\n"
      ],
      "metadata": {
        "id": "Cr44-vuvI5f3"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    batch_size = 50\n",
        "    # Pre-process the data, using create_test_val_set\n",
        "    reviews_encoded, labels_encoded, word2int = pre_process()\n",
        "    reviews_encoded, labels_ndarray = process_reviews(reviews_encoded, labels_encoded)\n",
        "    reviews_ndarray = pad_features(reviews_encoded, 200)\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = create_traing_test_val_set(reviews_ndarray, labels_ndarray)\n",
        "    train_loader, val_loader, test_loader = create_batches(train_x, train_y, val_x, val_y, test_x, test_y, batch_size)\n",
        "\n",
        "    #init mode\n",
        "    mode = \"PREDICTION\"  # Choose from TRAIN, TEST, PREDICTION\n",
        "\n",
        "    #model details\n",
        "    output_size = 1\n",
        "    embedding_dim = 1024\n",
        "    n_hidden = 512\n",
        "    n_layers = 4\n",
        "    model = SentimentRNN(word2int, embedding_dim, n_hidden, n_layers)\n",
        "\n",
        "    if mode == \"TRAIN\":\n",
        "        print(\"Training Mode\")\n",
        "        train(model, train_loader, val_loader, batch_size=batch_size, epochs=25)\n",
        "\n",
        "    if mode == \"PREDICTION\":\n",
        "        print(\"Prediction Mode\")\n",
        "        positive = \"I enjoyed the movie and the seats were comfy\"\n",
        "        negative = \"The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.\"\n",
        "        inputs = [positive, negative]\n",
        "        for input_text in inputs:\n",
        "            prediction_result = prediction(model, input_text)\n",
        "            print(prediction_result)\n",
        "\n",
        "    if mode == \"TEST\":\n",
        "        print(\"Testing Mode\")\n",
        "        test(model, test_loader)\n",
        "\n",
        "\n",
        "    #these conditional statments were inspired by my time coding in Java, which was famously known for having switch cases in GE's.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykNUhqrO4pXY",
        "outputId": "835de006-eca2-44b7-e3db-3dec3868785f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 1 reviews\n",
            "Prediction Mode\n",
            "Raw output (pre-sigmoid): 0.5027230978012085\n",
            "Prediction value, pre-rounding: 0.623099\n",
            "Positive\n",
            "Raw output (pre-sigmoid): 0.5018658638000488\n",
            "Prediction value, pre-rounding: 0.622898\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowsy results but I did my best, followed online pytorch LSTM implmentation, and watch a few YT videos. Wasnt the easiest thing ITW (in the world)\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(model.device)\n",
        "            labels = labels.to(model.device)\n",
        "            predictions, _ = model(inputs, model.init_hidden(inputs.size(0)))\n",
        "\n",
        "            predicted_labels = torch.round(predictions).cpu().numpy()\n",
        "            y_pred.extend(predicted_labels)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%') #2 decimal places bc why not\n",
        "    print(f'Precision: {precision*100:.2f}%')\n",
        "    print(f'Recall: {recall*100:.2f}%')\n",
        "    print(f'F1-Score: {f1*100:.2f}%')\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17U3cNOK5P4w",
        "outputId": "5292b593-c691-4ec2-b15a-b2257a41d666"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n",
            "Precision: 50.00%\n",
            "Recall: 100.00%\n",
            "F1-Score: 66.67%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5, 0.5, 1.0, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHu4sOfTNApp"
      },
      "execution_count": 81,
      "outputs": []
    }
  ]
}